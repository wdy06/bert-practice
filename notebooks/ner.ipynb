{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.cache/pypoetry/virtualenvs/bert-practice-mMzgu-b7-py3.8/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import random\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import unicodedata\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertJapaneseTokenizer, BertForTokenClassification\n",
    "import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"cl-tohoku/bert-base-japanese-whole-word-masking\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# representation of named entitiy recognition\n",
    "text = \"AさんはBCD株式会社を起業した。\"\n",
    "entities = [\n",
    "    {\"name\": \"A\", \"span\": [0, 1], \"type\": \"人名\", \"type_id\": 1},\n",
    "    {\"name\": \"BCD株式会社\", \"span\": [4, 11], \"type\": \"組織名\", \"type_id\": 1},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NER_tokenizer(BertJapaneseTokenizer):\n",
    "    def encode_plus_tagged(self, text, entities, max_length):\n",
    "        \"\"\"\n",
    "        文章とそれに含まれる固有表現が与えられたときに、\n",
    "        符号化とラベル列の作成を行う\n",
    "        \"\"\"\n",
    "\n",
    "        # 固有表現の前後でテキストを分割し、それぞれのラベルをつけておく\n",
    "        entities = sorted(entities, key=lambda x: x['span'][0])\n",
    "        splitted = []\n",
    "        position = 0\n",
    "        for entity in entities:\n",
    "            start = entity['span'][0]\n",
    "            end = entity[\"span\"][1]\n",
    "            label = entity['type_id']\n",
    "            # この分割方法は固有表現が連続して存在する場合には対応できない\n",
    "            # 固有表現でないもの\n",
    "            splitted.append({'text': text[position:start], 'label': 0})\n",
    "            # 固有表現には対応するラベルを付与\n",
    "            splitted.append({'text': text[start:end], 'label': label})\n",
    "            position = end\n",
    "        splitted.append({'text': text[position:], 'label': 0})\n",
    "        splitted = [s for s in splitted if s[\"text\"]] # remove empty string\n",
    "\n",
    "        # tokenize splitted string and add label\n",
    "        tokens = []\n",
    "        labels = []\n",
    "        for text_splitted in splitted:\n",
    "            text = text_splitted['text']\n",
    "            label = text_splitted[\"label\"]\n",
    "            tokens_splitted = self.tokenize(text)\n",
    "            labels_splitted = [label] * len(tokens_splitted)\n",
    "            tokens.extend(tokens_splitted)\n",
    "            labels.extend(labels_splitted)\n",
    "        \n",
    "        input_ids = self.convert_tokens_to_ids(tokens)\n",
    "        encoding = self.prepare_for_model(\n",
    "            input_ids,\n",
    "            max_length=max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True\n",
    "\n",
    "        )\n",
    "        # make label 0 of special token [CLS], [SEP]\n",
    "        labels = [0] + labels[:max_length-2] + [0]\n",
    "        # make label 0 of special token [PAD]\n",
    "        labels = labels + [0]*(max_length - len(labels))\n",
    "        encoding['labels'] = labels\n",
    "        return encoding\n",
    "\n",
    "    def encode_plus_untagged(self, text, max_length=None, return_tensors=None):\n",
    "        \"\"\"\n",
    "        文章をトークン化し、それぞれのトークンの文章中の位置も特定しておく\n",
    "        \"\"\"\n",
    "        # 文章のトークン化を行い、\n",
    "        # それぞれのトークンと文章中の文字列を対応づける\n",
    "        tokens = []\n",
    "        tokens_original = [] # トークンに対応する文章中の文字列\n",
    "        words = self.word_tokenizer.tokenize(text) # splitted by MeCab\n",
    "        for word in words:\n",
    "            # split word to sub word\n",
    "            tokens_word = self.subword_tokenizer.tokenize(word)\n",
    "            tokens.extend(tokens_word)\n",
    "            if tokens_word[0] == \"[UNK]\":\n",
    "                tokens_original.append(word)\n",
    "            else:\n",
    "                tokens_original.extend([token.replace(\"##\", \"\") for token in tokens_word])\n",
    "        \n",
    "        # 各トークンの文章中での位置を調べる。（空白の位置を考慮する）\n",
    "        position = 0\n",
    "        spans = [] # トークンの位置を追加していく。\n",
    "        for token in tokens_original:\n",
    "            token_length = len(token)\n",
    "            while 1:\n",
    "                if token != text[position:position+token_length]:\n",
    "                    position += 1\n",
    "                else:\n",
    "                    spans.append([position, position+token_length])\n",
    "                    position += token_length\n",
    "                    break\n",
    "\n",
    "        # 符号化を行いBERTに入力できる形式にする。\n",
    "        input_ids = self.convert_tokens_to_ids(tokens) \n",
    "        encoding = self.prepare_for_model(\n",
    "            input_ids, \n",
    "            max_length=max_length, \n",
    "            padding='max_length' if max_length else False, \n",
    "            truncation=True if max_length else False\n",
    "        )\n",
    "        sequence_length = len(encoding['input_ids'])\n",
    "        # 特殊トークン[CLS]に対するダミーのspanを追加。\n",
    "        spans = [[-1, -1]] + spans[:sequence_length-2] \n",
    "        # 特殊トークン[SEP]、[PAD]に対するダミーのspanを追加。\n",
    "        spans = spans + [[-1, -1]] * ( sequence_length - len(spans) ) \n",
    "\n",
    "        # 必要に応じてtorch.Tensorにする。\n",
    "        if return_tensors == 'pt':\n",
    "            encoding = { k: torch.tensor([v]) for k, v in encoding.items() }\n",
    "\n",
    "        return encoding, spans\n",
    "    \n",
    "    def convert_bert_output_to_entities(self, text, labels, spans):\n",
    "        \"\"\"\n",
    "        文章、ラベル列の予測値、各トークンの位置から固有表現を得る。\n",
    "        \"\"\"\n",
    "        # labels, spansから特殊トークンに対応する部分を取り除く\n",
    "        labels = [label for label, span in zip(labels, spans) if span[0] != -1]\n",
    "        spans = [span for span in spans if span[0] != -1]\n",
    "\n",
    "        # 同じラベルが連続するトークンをまとめて、固有表現を抽出する。\n",
    "        entities = []\n",
    "        for label, group \\\n",
    "            in itertools.groupby(enumerate(labels), key=lambda x: x[1]):\n",
    "            group = list(group)\n",
    "            # print(f\"label: {label}, group: {group}\")\n",
    "            start = spans[group[0][0]][0]\n",
    "            end = spans[group[-1][0]][1]\n",
    "\n",
    "            if label != 0: # ラベルが0以外ならば、新たな固有表現として追加。\n",
    "                entity = {\n",
    "                    \"name\": text[start:end],\n",
    "                    \"span\": [start, end],\n",
    "                    \"type_id\": label\n",
    "                }\n",
    "                entities.append(entity)\n",
    "\n",
    "        return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertJapaneseTokenizer'. \n",
      "The class this function is called from is 'NER_tokenizer'.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = NER_tokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [2, 10271, 28486, 5, 546, 10780, 2464, 13, 5, 1878, 2682, 9, 10750, 308, 10, 8, 3, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0], 'labels': [0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "text = '昨日のみらい事務所との打ち合わせは順調だった。'\n",
    "entities = [\n",
    "    {'name': 'みらい事務所', 'span': [3,9], 'type_id': 1}\n",
    "]\n",
    "\n",
    "encoding = tokenizer.encode_plus_tagged(\n",
    "    text, entities, max_length=20\n",
    ")\n",
    "print(encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# encoding\n",
      "{'input_ids': tensor([[    2,     1, 26280,     5,  1543,   125,     9,  6749, 28550,  2953,\n",
      "         28550, 28566, 21202, 28683, 14050, 12475,    12,    31,     8,     3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "# spans\n",
      "[[-1, -1], [0, 1], [1, 2], [2, 3], [3, 5], [5, 6], [6, 7], [7, 9], [9, 10], [10, 12], [12, 13], [13, 14], [15, 18], [18, 19], [19, 23], [24, 27], [27, 28], [28, 30], [30, 31], [-1, -1]]\n"
     ]
    }
   ],
   "source": [
    "text = '騰訊の英語名はTencent Holdings Ltdである。'\n",
    "encoding, spans = tokenizer.encode_plus_untagged(\n",
    "    text, return_tensors='pt'\n",
    ")\n",
    "print('# encoding')\n",
    "print(encoding)\n",
    "print('# spans')\n",
    "print(spans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label: 1, group: [(0, 1), (1, 1)]\n",
      "label: 0, group: [(2, 0), (3, 0), (4, 0), (5, 0)]\n",
      "label: 1, group: [(6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1), (14, 1)]\n",
      "label: 0, group: [(15, 0), (16, 0), (17, 0)]\n",
      "[{'name': '騰訊', 'span': [0, 2], 'type_id': 1}, {'name': 'Tencent Holdings Ltd', 'span': [7, 27], 'type_id': 1}]\n"
     ]
    }
   ],
   "source": [
    "labels_predicted = [0,1,1,0,0,0,0,1,1,1,1,1,1,1,1,1,0,0,0,0]\n",
    "entities = tokenizer.convert_bert_output_to_entities(\n",
    "    text, labels_predicted, spans\n",
    ")\n",
    "print(entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertJapaneseTokenizer'. \n",
      "The class this function is called from is 'NER_tokenizer'.\n",
      "Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# BERTによる固有表現抽出\n",
    "tokenizer = NER_tokenizer.from_pretrained(MODEL_NAME)\n",
    "bert_tc = BertForTokenClassification.from_pretrained(MODEL_NAME, num_labels=4)\n",
    "bert_tc = bert_tc.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[   2,  192, 2375,    9,  277,  396,    7, 2663,   15,   10,    8,    3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "[[-1, -1], [0, 1], [1, 3], [3, 4], [4, 5], [5, 7], [7, 8], [8, 10], [10, 11], [11, 12], [12, 13], [-1, -1]]\n",
      "label: 0, group: [(0, 0), (1, 0)]\n",
      "label: 3, group: [(2, 3), (3, 3)]\n",
      "label: 0, group: [(4, 0)]\n",
      "label: 1, group: [(5, 1)]\n",
      "label: 0, group: [(6, 0)]\n",
      "label: 1, group: [(7, 1), (8, 1), (9, 1)]\n",
      "[{'name': 'はB', 'span': [3, 5], 'type_id': 3}, {'name': 'に', 'span': [7, 8], 'type_id': 1}, {'name': 'した。', 'span': [10, 13], 'type_id': 1}]\n"
     ]
    }
   ],
   "source": [
    "text = \"AさんはB大学に入学した。\"\n",
    "\n",
    "encoding, spans = tokenizer.encode_plus_untagged(text, return_tensors='pt')\n",
    "\n",
    "print(encoding)\n",
    "print(spans)\n",
    "\n",
    "encoding = {k: v.cuda() for k, v in encoding.items()}\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = bert_tc(**encoding)\n",
    "    scores = output.logits\n",
    "    labels_predicted = scores[0].argmax(-1).cpu().numpy().tolist()\n",
    "\n",
    "entities = tokenizer.convert_bert_output_to_entities(text, labels_predicted, spans)\n",
    "print(entities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    {\n",
    "        'text': 'AさんはB大学に入学した。',\n",
    "        'entities': [\n",
    "            {'name': 'A', 'span': [0, 1], 'type_id': 2},\n",
    "            {'name': 'B大学', 'span': [4, 7], 'type_id': 1}\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        'text': 'CDE株式会社は新製品「E」を販売する。',\n",
    "        'entities': [\n",
    "            {'name': 'CDE株式会社', 'span': [0, 7], 'type_id': 1},\n",
    "            {'name': 'E', 'span': [12, 13], 'type_id': 3}\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "max_length = 32\n",
    "dataset_for_loader = []\n",
    "for sample in data:\n",
    "    text = sample[\"text\"]\n",
    "    entities = sample['entities']\n",
    "    encoding = tokenizer.encode_plus_tagged(text, entities, max_length=max_length)\n",
    "    encoding = {k: torch.tensor(v) for k, v in encoding.items()}\n",
    "    dataset_for_loader.append(encoding)\n",
    "dataloader = DataLoader(dataset_for_loader, batch_size=len(data))\n",
    "\n",
    "for batch in dataloader:\n",
    "    batch = {k: v.cuda() for k, v in batch.items()}\n",
    "    output = bert_tc(**batch)\n",
    "    loss = output.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nerのfinetuning wikiのデータを使う\n",
    "dataset = json.load(open(\"../data/ner-wikipedia-dataset/ner.json\", \"r\"))\n",
    "\n",
    "type_id_dict = {\n",
    "    \"人名\": 1, \n",
    "    \"法人名\": 2,\n",
    "    \"政治的組織名\": 3,\n",
    "    \"その他の組織名\": 4,\n",
    "    \"地名\": 5,\n",
    "    \"施設名\": 6,\n",
    "    \"製品名\": 7,\n",
    "    \"イベント名\": 8,\n",
    "}\n",
    "\n",
    "for sample in dataset:\n",
    "    sample['text'] = unicodedata.normalize(\"NFKC\", sample[\"text\"])\n",
    "    for e in sample['entities']:\n",
    "        e['type_id'] = type_id_dict[e[\"type\"]]\n",
    "        del e['type']\n",
    "\n",
    "random.shuffle(dataset)\n",
    "n = len(dataset)\n",
    "n_train = int(n*0.6)\n",
    "n_val = int(n*0.2)\n",
    "dataset_train = dataset[:n_train]\n",
    "dataset_val = dataset[n_train:n_train+n_val]\n",
    "dataset_test = dataset[n_train+n_val:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertJapaneseTokenizer'. \n",
      "The class this function is called from is 'NER_tokenizer'.\n"
     ]
    }
   ],
   "source": [
    "def create_dataset(tokenizer, dataset, max_length):\n",
    "    dataset_for_loader = []\n",
    "    for sample in dataset:\n",
    "        text = sample[\"text\"]\n",
    "        entities = sample[\"entities\"]\n",
    "        encoding = tokenizer.encode_plus_tagged(text, entities, max_length)\n",
    "        encoding = {k: torch.tensor(v) for k, v in encoding.items()}\n",
    "        dataset_for_loader.append(encoding)\n",
    "    return dataset_for_loader\n",
    "\n",
    "tokenizer = NER_tokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "max_length = 128\n",
    "dataset_train_for_loader = create_dataset(tokenizer, dataset_train, max_length)\n",
    "dataset_val_for_loader = create_dataset(tokenizer, dataset_val, max_length)\n",
    "\n",
    "dataloader_train = DataLoader(dataset_train_for_loader, batch_size=32, shuffle=True)\n",
    "dataloader_val = DataLoader(dataset_val_for_loader, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.cache/pypoetry/virtualenvs/bert-practice-mMzgu-b7-py3.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:441: LightningDeprecationWarning: Setting `Trainer(gpus=1)` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=1)` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/jovyan/.cache/pypoetry/virtualenvs/bert-practice-mMzgu-b7-py3.8/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:604: UserWarning: Checkpoint directory /home/jovyan/data/micky/workspace/github.com/wdy06/bert-practice/model exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name    | Type                       | Params\n",
      "-------------------------------------------------------\n",
      "0 | bert_tc | BertForTokenClassification | 110 M \n",
      "-------------------------------------------------------\n",
      "110 M     Trainable params\n",
      "0         Non-trainable params\n",
      "110 M     Total params\n",
      "440.135   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.cache/pypoetry/virtualenvs/bert-practice-mMzgu-b7-py3.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.cache/pypoetry/virtualenvs/bert-practice-mMzgu-b7-py3.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 106/106 [02:44<00:00,  1.55s/it, loss=0.0154, v_num=5]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=5` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 106/106 [02:49<00:00,  1.60s/it, loss=0.0154, v_num=5]\n"
     ]
    }
   ],
   "source": [
    "class BertForTokenClassification_pl(pl.LightningModule):\n",
    "    def __init__(self, model_name, num_labels, lr):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.bert_tc = BertForTokenClassification.from_pretrained(model_name, num_labels=num_labels)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        output = self.bert_tc(**batch)\n",
    "        loss = output.loss\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        output = self.bert_tc(**batch)\n",
    "        val_loss = output.loss\n",
    "        self.log('val_loss', val_loss)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.hparams.lr)\n",
    "\n",
    "checkpoint = pl.callbacks.ModelCheckpoint(\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    save_top_k=1,\n",
    "    save_weights_only=True,\n",
    "    dirpath = '../model/'\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    gpus=1,\n",
    "    max_epochs=5,\n",
    "    callbacks=[checkpoint]\n",
    ")\n",
    "\n",
    "model = BertForTokenClassification_pl(MODEL_NAME, num_labels=9, lr=1e-5)\n",
    "trainer.fit(model, dataloader_train, dataloader_val)\n",
    "best_model_path = checkpoint.best_model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertJapaneseTokenizer'. \n",
      "The class this function is called from is 'NER_tokenizer'.\n",
      "Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 1070/1070 [00:25<00:00, 42.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# ground truth\n",
      "[{'name': '男子B.LEAGUE', 'span': [3, 13], 'type_id': 4}, {'name': 'アルバルク東京', 'span': [18, 25], 'type_id': 4}]\n",
      "# extract\n",
      "[{'name': '男子B.LEAGUE', 'span': [3, 13], 'type_id': 4}, {'name': 'アルバルク東京', 'span': [18, 25], 'type_id': 4}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# inference\n",
    "\n",
    "def predict(text, tokenizer, bert_tc):\n",
    "    encoding, spans = tokenizer.encode_plus_untagged(\n",
    "        text, return_tensors='pt'\n",
    "    )\n",
    "    encoding = {k: v.cuda() for k, v in encoding.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = bert_tc(**encoding)\n",
    "        scores = output.logits\n",
    "        labels_predicted = scores[0].argmax(-1).cpu().numpy().tolist()\n",
    "\n",
    "    entities = tokenizer.convert_bert_output_to_entities(\n",
    "        text, labels_predicted, spans\n",
    "    )\n",
    "\n",
    "    return entities\n",
    "\n",
    "tokenizer = NER_tokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "model = BertForTokenClassification_pl.load_from_checkpoint(best_model_path)\n",
    "\n",
    "bert_tc = model.bert_tc.cuda()\n",
    "\n",
    "# コードのわかりやすさのために1データずつ処理しているが、\n",
    "# バッチで処理したほうが早い\n",
    "\n",
    "entities_list = []\n",
    "entities_predicted_list = []\n",
    "for sample in tqdm(dataset_test):\n",
    "    text = sample['text']\n",
    "    entities_predicted = predict(text, tokenizer, bert_tc)\n",
    "    entities_list.append(sample['entities'])\n",
    "    entities_predicted_list.append(entities_predicted)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# ground truth\n",
      "[{'name': 'AIWAF', 'span': [0, 5], 'type_id': 7}]\n",
      "# extract\n",
      "[{'name': 'AIWAF', 'span': [0, 5], 'type_id': 7}]\n"
     ]
    }
   ],
   "source": [
    "print(\"# ground truth\")\n",
    "print(entities_list[2])\n",
    "print(\"# extract\")\n",
    "print(entities_predicted_list[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bert-practice-mMzgu-b7-py3.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "23326a8809963bbbc8835b1c317450501e95baa1b9baebc29fb0a4f3937077e5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
